{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "yelpToWebClassification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPVe3kal0kQjD49Y4DSGTWs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacob-hansen/Multimodal-Activity-Classification/blob/main/Classification/yelpToWebClassification-Jacob.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Open Connection to Google Sheets File"
      ],
      "metadata": {
        "id": "P3LtbDQJOO-Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r_NH0kbONuDM"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "worksheet = gc.open('Yelp Scraping Data Final').sheet1\n",
        "places = set(worksheet.col_values(2)[1:])\n",
        "testValues = worksheet.get()\n",
        "headerNames = worksheet.get(\"A1:F1\")[0]\n",
        "headerDic = {headerNames[i]: i for i in range(len(headerNames))}"
      ],
      "metadata": {
        "id": "I3o_vuCGN2jY"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filter Text to Desired Words"
      ],
      "metadata": {
        "id": "HCcFKrosOXtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up nltk imports for tokenization\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('regexp')\n",
        "from nltk.tokenize import RegexpTokenizer"
      ],
      "metadata": {
        "id": "FzGQCwbaPzfG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e823fc8-4e16-4225-d3b6-0f2273e22429"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Error loading regexp: Package 'regexp' not found in index\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up nltk and gensim stopwords removal\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.parsing.preprocessing import STOPWORDS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sYg0Qk7qWmR",
        "outputId": "a0f632fc-2c28-477a-e691-9229ec0e600f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set up to get words to stem root\n",
        "import re\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")"
      ],
      "metadata": {
        "id": "rr3gKxKPkhSj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getTextByRating(file, minRating, readLength = 0):\n",
        "    \"\"\"\n",
        "    From a specified Sheets File in Google Drive, \n",
        "    get the readLength number of reviews and return those that have the given minRating. \n",
        "    If readLength == 0, default is returning all reviews. \n",
        "    \"\"\"\n",
        "    worksheet = gc.open(file).sheet1\n",
        "    if readLength != 0:\n",
        "        zipped = zip(worksheet.col_values(3)[1:readLength], worksheet.col_values(1)[1:readLength], worksheet.col_values(4)[1:readLength])\n",
        "    else:\n",
        "        zipped = zip(worksheet.col_values(3)[1:], worksheet.col_values(1)[1:], worksheet.col_values(4)[1:])\n",
        "    places = (i[1:] for i in zipped if int(i[0]) >= minRating)\n",
        "    activityNames = {}\n",
        "    for row in testValues:\n",
        "        if row[0] not in activityNames:\n",
        "            activityNames[row[0]] = row[1]\n",
        "    return places, activityNames"
      ],
      "metadata": {
        "id": "NTdoYxzkVTKb"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def masterTokenizer(inputtext):\n",
        "    no_nums = re.sub(r'[0-9]+', ' ', inputtext)\n",
        "    firstTokens = tokenizer.tokenize(no_nums) \n",
        "    stemmed_words_ps = [stemmer.stem(word) for word in firstTokens]\n",
        "    tokens_without_sw = [word.lower() for word in stemmed_words_ps if word.lower() not in STOPWORDS and word.lower() not in stopwords.words('english')]\n",
        "    return list(tokens_without_sw)"
      ],
      "metadata": {
        "id": "Qke384BJKAbF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testText, activityNames = getTextByRating('Yelp Scraping Data Final', 4)\n",
        "tokenList = {}\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "for activity, review in testText:\n",
        "    if int(activity) not in tokenList:\n",
        "        tokenList[int(activity)] = []\n",
        "    if len(review) > 200:\n",
        "        tokenList[int(activity)].append(masterTokenizer(review)) \n",
        "        # python maintains dictionary key orders, \n",
        "        # So tokenList.keys() will return the correct order of insertion\n",
        "    "
      ],
      "metadata": {
        "id": "XM-RaTFXR4-K"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "output tokens is tokenList (a dictionary of activities 0-n of a list of list of words)"
      ],
      "metadata": {
        "id": "DNze9zYroezX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gensim Word2Vec "
      ],
      "metadata": {
        "id": "SEXZwRVQqfx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import itertools\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Qvdy5g01qiiP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getVec(model, inputtext): \n",
        "    modifiedText = masterTokenizer(inputtext)\n",
        "    checkText = [i for i in modifiedText if i in model.wv]\n",
        "    vTx = [0]*model.vector_size\n",
        "    for word in checkText:\n",
        "        vTx += np.array(model[word])\n",
        "    vTNorm = vTx / np.sqrt(np.dot(vTx,vTx))\n",
        "    return vTNorm"
      ],
      "metadata": {
        "id": "kkFBdj9TLmgY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# groups = [j.join(' ') for i in tokenList for j in tokenList[i]]\n",
        "sentences = [list(itertools.chain.from_iterable(tokenList[i])) for i in tokenList]\n",
        "WVmodel = Word2Vec(sentences, min_count=3, size=10, window=200)\n",
        "vocabulary = WVmodel.wv\n",
        "activityVecs = np.array([getVec(WVmodel, \" \".join(i)) for i in sentences])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBTo7tneqjxx",
        "outputId": "e9d4f23f-0f78-4ff7-b6bc-7393dba3b874"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def inf_norm(matrix):\n",
        "    return matrix"
      ],
      "metadata": {
        "id": "Q8wnautWO0Cu"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mostProbable(model, activityVecs, inputText, activityNames = False):\n",
        "    inputVec = np.matrix(getVec(model, inputText))\n",
        "    if activityNames:\n",
        "        return activityNames[str(np.argmax(inf_norm(activityVecs*np.transpose(inputVec))))]\n",
        "    else:\n",
        "        return inf_norm(activityVecs*np.transpose(inputVec))\n"
      ],
      "metadata": {
        "id": "_tBTbql8OQOz"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testSentence = \"I love going to the aquarium. I get to see tons of fish and I get to explore. Every floor is so cool! There are penguins, frogs, more fish. You name it!\"\n",
        "test2 = \"Escape rooms that I got out fast\"  #Yesterday I went to the escape room with some friends. There were a lot of us in the room, so we got out pretty fast. But there were fun puzzles and tricks.\"\n",
        "print(mostProbable(WVmodel, activityVecs, testSentence, activityNames))\n",
        "print(mostProbable(WVmodel, activityVecs, test2, activityNames))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o58HL8UdMzaO",
        "outputId": "6238bcb1-3976-40c5-8677-b23a113ea5fd"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new england aquarium boston\n",
            "boxaroo boston 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reviewSimilarity(model, a, b): \n",
        "    \"\"\"\n",
        "    Given a gensim Word2Vec Model\n",
        "    Finds and sums the vectors for each word in each sentence\n",
        "    Computes and returns the Cosine Distance\n",
        "    \"\"\"\n",
        "    checkA = [i for i in a if i in model.wv]\n",
        "    checkB = [i for i in b if i in model.wv]\n",
        "    vA = [0]*model.vector_size\n",
        "    vB = [0]*model.vector_size\n",
        "    for word in checkA:\n",
        "        vA += np.array(model[word])\n",
        "    for word in checkB:\n",
        "        vB += np.array(model[word])\n",
        "    distance = abs(np.dot(vA, vB) / (np.sqrt(np.dot(vA,vA)) * np.sqrt(np.dot(vB,vB))))\n",
        "    return distance \n"
      ],
      "metadata": {
        "id": "ah_m8NxPFax3"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviewSimilarity(WVmodel, tokenList[0][4], tokenList[0][24])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lt-6lgOr_6t5",
        "outputId": "90fe2c81-8bfe-4cb8-ab2d-fb4259646a26"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9958994746097066"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testActivity1 = \"The interactive exhibits and exquisite attention to historical detail make this a quintessential Boston museum that every visitor must experience.\"  \n",
        "testActivity4 = \"Voted one of the Best Boston Ghost Tours for a Frightfully Good Time that's guaranteed to raise your spirits. Not all Boston Haunted Tours are created equal.\"\n",
        "print(reviewSimilarity(WVmodel, testActivity1, testActivity4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnE9D7KKEd6J",
        "outputId": "c0d97ae0-1872-443d-c1a9-16d7ba837bf6"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.2607496960179214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Google Web Searches Against Review Sorting"
      ],
      "metadata": {
        "id": "1VHOrXPeewir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "worksheet = gc.open('Google Search Websites Boston').sheet1\n",
        "testValues = worksheet.get()[1:]"
      ],
      "metadata": {
        "id": "lTZKbX51K0O1"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = {}\n",
        "for row in testValues:\n",
        "    try:\n",
        "      activity = row[0]\n",
        "      if activity not in scores:\n",
        "          scores[activity] = []\n",
        "      text = row[2]+\" \"+row[3]\n",
        "      if len(row) == 6:\n",
        "        text += \" \"+row[5]\n",
        "      test = mostProbable(WVmodel, activityVecs, text)\n",
        "      scores[activity].append(str(np.argmax(test))==activity)\n",
        "    except:\n",
        "      print(len(row))\n",
        "      continue\n",
        "total = [0, 0]\n",
        "for i in scores:\n",
        "  subset = scores[i][:4]\n",
        "  total[0] += sum(subset)\n",
        "  total[1] += len(subset)\n",
        "  print(\"Correctly predicted \"+str(sum(subset))+\" of \"+str(len(subset))+\" websites for \"+str(activityNames[i]))\n",
        "print(\"Predicting success total at \"+str(total[0]/total[1]*100)+\"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szhED6RHgCNY",
        "outputId": "057fd87a-ff59-4b9b-94d8-28b8b9ae6691"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correctly predicted 4 of 4 websites for boston tea party ships and museum boston\n",
            "Correctly predicted 0 of 4 websites for trapology boston boston\n",
            "Correctly predicted 2 of 4 websites for jacques cabaret boston\n",
            "Correctly predicted 4 of 4 websites for haunted boston ghost tours boston\n",
            "Correctly predicted 1 of 4 websites for escape the room boston boston\n",
            "Correctly predicted 0 of 4 websites for the lawn on d boston\n",
            "Correctly predicted 3 of 4 websites for charles river canoe and kayak cambridge 2\n",
            "Correctly predicted 2 of 4 websites for urban axes somerville somerville\n",
            "Correctly predicted 1 of 4 websites for cambridge center roof garden cambridge\n",
            "Correctly predicted 2 of 4 websites for lucky strike somerville somerville\n",
            "Correctly predicted 4 of 4 websites for the esplanade boston 2\n",
            "Correctly predicted 3 of 4 websites for boxaroo boston 6\n",
            "Correctly predicted 1 of 4 websites for chez vous roller skating rink boston\n",
            "Predicting success total at 51.92307692307693%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Explanation\n",
        "The model predicted around half of the websites correctly. Given the limited data set we have, I was happy with the results (the model only took 10 sec to train). Obviously, the biggest limitation in this model is the vocabulary. Many of the words in non-training samples are not found in the vocabulary. Additionally, with limited data, it is especially hard to make predictions on data formated differently than the training data. In this case, I simply concatenated all the information provided by Google for each website. Ideally, I would attempt this again by training on a variety of information and preclassify like activities. In the training set, there were 3 escape room activities. It's no wonder that the model preformed poorly on most of those activities. Also, the descriptions of the lawn on boston and cambridge center roof garden are difficult to distinguish (even by hand once names were taken out). \n",
        "\n",
        "In a model attempting to classify activities from people's lives, it will be important to get a time and location stamp to help strengthen activities that should be grouped together. I propose first collecting a substantial database of journals and information relating to activites of those people who journaled. Then I would first group information by location and time. I would further train a model simply for weeding out non-similar data. Then I would train a seperate model for recognizing similar type data. Importantly, the two approaches for cleaning the data and then training on the final model will need to be different and require more thought. "
      ],
      "metadata": {
        "id": "Wv2ZHpLMtgMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mq5IMi4xhPrZ"
      },
      "execution_count": 72,
      "outputs": []
    }
  ]
}