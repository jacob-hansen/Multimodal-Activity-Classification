{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\evely\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "from IPython import display as ipythondisplay\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2225 [[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n",
      "2225 ['tv', 'future', 'hands', 'viewers', 'home', 'theatre', 'systems', 'plasma', 'high', 'definition', 'tvs', 'digital', 'video', 'recorders', 'moving', 'living', 'room', 'way', 'people', 'watch', 'tv', 'radically', 'different', 'five', 'years', 'time', 'according', 'expert', 'panel', 'gathered', 'annual', 'consumer', 'electronics', 'show', 'las', 'vegas', 'discuss', 'new', 'technologies', 'impact', 'one', 'favourite', 'pastimes', 'us', 'leading', 'trend', 'programmes', 'content', 'delivered', 'viewers', 'via', 'home', 'networks', 'cable', 'satellite', 'telecoms', 'companies', 'broadband', 'service', 'providers', 'front', 'rooms', 'portable', 'devices', 'one', 'talked', 'about', 'technologies', 'ces', 'digital', 'personal', 'video', 'recorders', 'dvr', 'pvr', 'set', 'top', 'boxes', 'like', 'us', 'tivo', 'uk', 'sky+', 'system', 'allow', 'people', 'record', 'store', 'play', 'pause', 'forward', 'wind', 'tv', 'programmes', 'want', 'essentially', 'technology', 'allows', 'much', 'personalised', 'tv', 'also', 'built', 'in', 'high', 'definition', 'tv', 'sets', 'big', 'business', 'japan', 'us', 'slower', 'take', 'europe', 'lack', 'high', 'definition', 'programming', 'people', 'forward', 'wind', 'adverts', 'also', 'forget', 'abiding', 'network', 'channel', 'schedules', 'putting', 'together', 'a', 'la', 'carte', 'entertainment', 'us', 'networks', 'cable', 'satellite', 'companies', 'worried', 'means', 'terms', 'advertising', 'revenues', 'well', 'brand', 'identity', 'viewer', 'loyalty', 'channels', 'although', 'us', 'leads', 'technology', 'moment', 'also', 'concern', 'raised', 'europe', 'particularly', 'growing', 'uptake', 'services', 'like', 'sky+', 'happens', 'today', 'see', 'nine', 'months', 'years', 'time', 'uk', 'adam', 'hume', 'bbc', 'broadcast', 'futurologist', 'told', 'bbc', 'news', 'website', 'likes', 'bbc', 'issues', 'lost', 'advertising', 'revenue', 'yet', 'pressing', 'issue', 'moment', 'commercial', 'uk', 'broadcasters', 'brand', 'loyalty', 'important', 'everyone', 'talking', 'content', 'brands', 'rather', 'network', 'brands', 'said', 'tim', 'hanlon', 'brand', 'communications', 'firm', 'starcom', 'mediavest', 'reality', 'broadband', 'connections', 'anybody', 'producer', 'content', 'added', 'challenge', 'hard', 'promote', 'programme', 'much', 'choice', 'means', 'said', 'stacey', 'jolna', 'senior', 'vice', 'president', 'tv', 'guide', 'tv', 'group', 'way', 'people', 'find', 'content', 'want', 'watch', 'simplified', 'tv', 'viewers', 'means', 'networks', 'us', 'terms', 'channels', 'could', 'take', 'leaf', 'google', 'book', 'search', 'engine', 'future', 'instead', 'scheduler', 'help', 'people', 'find', 'want', 'watch', 'kind', 'channel', 'model', 'might', 'work', 'younger', 'ipod', 'generation', 'used', 'taking', 'control', 'gadgets', 'play', 'them', 'might', 'suit', 'everyone', 'panel', 'recognised', 'older', 'generations', 'comfortable', 'familiar', 'schedules', 'channel', 'brands', 'know', 'getting', 'perhaps', 'want', 'much', 'choice', 'put', 'hands', 'mr', 'hanlon', 'suggested', 'end', 'kids', 'diapers', 'pushing', 'buttons', 'already', 'everything', 'possible', 'available', 'said', 'mr', 'hanlon', 'ultimately', 'consumer', 'tell', 'market', 'want', '50', '000', 'new', 'gadgets', 'technologies', 'showcased', 'ces', 'many', 'enhancing', 'tv', 'watching', 'experience', 'high', 'definition', 'tv', 'sets', 'everywhere', 'many', 'new', 'models', 'lcd', 'liquid', 'crystal', 'display', 'tvs', 'launched', 'dvr', 'capability', 'built', 'instead', 'external', 'boxes', 'one', 'example', 'launched', 'show', 'humax', '26', 'inch', 'lcd', 'tv', '80', 'hour', 'tivo', 'dvr', 'dvd', 'recorder', 'one', 'us', 'biggest', 'satellite', 'tv', 'companies', 'directtv', 'even', 'launched', 'branded', 'dvr', 'show', '100', 'hours', 'recording', 'capability', 'instant', 'replay', 'search', 'function', 'set', 'pause', 'rewind', 'tv', '90', 'hours', 'microsoft', 'chief', 'bill', 'gates', 'announced', 'pre', 'show', 'keynote', 'speech', 'partnership', 'tivo', 'called', 'tivotogo', 'means', 'people', 'play', 'recorded', 'programmes', 'windows', 'pcs', 'mobile', 'devices', 'reflect', 'increasing', 'trend', 'freeing', 'multimedia', 'people', 'watch', 'want', 'want']\n"
     ]
    }
   ],
   "source": [
    "articles = []\n",
    "labels = []\n",
    "label_dict = {}\n",
    "reverse_label_dict = {}\n",
    "\n",
    "label_ind = 0\n",
    "with open(\"bbc-text.csv\", 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        if row[0] not in label_dict:\n",
    "            label_dict[row[0]] = label_ind\n",
    "            reverse_label_dict[label_ind] = row[0]\n",
    "            label_ind += 1\n",
    "\n",
    "        labels.append(label_dict[row[0]])\n",
    "        article = row[1]\n",
    "        for word in STOPWORDS:\n",
    "            token = ' ' + word + ' '\n",
    "            article = article.replace(token, ' ')\n",
    "            article = article.replace(' ', ' ')\n",
    "        articles.append([word for word in re.split(',|_|-|\\!| |\\.|\\(|\\)|\\:|\\?', article) if word != ''])\n",
    "\n",
    "a = np.array(labels)\n",
    "b = np.zeros((a.size, a.max()+1))\n",
    "b[np.arange(a.size),a] = 1\n",
    "labels = b\n",
    "\n",
    "print(len(labels), labels[:10])\n",
    "print(len(articles), articles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=30495, vector_size=100, alpha=0.025)\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(articles, min_count=1)\n",
    "print(model)\n",
    "words = list(model.wv.key_to_index.keys())\n",
    "# print(words)\n",
    "# print(model.wv['mr'])\n",
    "# save model\n",
    "print(model.vector_size)\n",
    "model.save('model.bin')\n",
    "# load model\n",
    "# new_model = Word2Vec.load('model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "embedding_dim = 64\n",
    "max_length = 200\n",
    "padding_value = np.zeros(model.vector_size)\n",
    "training_portion = .8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(articles) * training_portion)\n",
    "\n",
    "train_articles = articles[0: train_size]\n",
    "train_sequences = [[model.wv[word] for word in article] for article in train_articles]\n",
    "train_labels = labels[0: train_size]\n",
    "\n",
    "validation_articles = articles[train_size:]\n",
    "validation_sequences = [[model.wv[word] for word in article] for article in validation_articles]\n",
    "validation_labels = labels[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(x, maxlen, value):\n",
    "    new_x = np.array([np.array([value for w in range(maxlen)]) for a in x])\n",
    "    for i in range(len(x)):\n",
    "        if len(x[i]) <= maxlen:\n",
    "            new_x[i, :len(x[i])] = np.array(x[i])\n",
    "        else:\n",
    "            new_x[i] = np.array(x[i][:maxlen])\n",
    "    return new_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1780, 200, 100)\n"
     ]
    }
   ],
   "source": [
    "train_padded = pad(train_sequences, maxlen=max_length, value=padding_value)\n",
    "validation_padded = pad(validation_sequences, maxlen=max_length, value=padding_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(articles, labels, batch_size):\n",
    "    idx = np.random.choice(len(articles), batch_size)\n",
    "\n",
    "    x_batch = np.array([articles[i] for i in idx])\n",
    "    y_batch = np.array([labels[i] for i in idx])\n",
    "  \n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(rnn_units, dense_units, output_units):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(rnn_units)),\n",
    "        tf.keras.layers.Dense(dense_units, activation='relu'),\n",
    "        tf.keras.layers.Dense(output_units, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(labels, logits):\n",
    "  loss = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True) # TODO\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization parameters:\n",
    "num_training_iterations = 2000  # Increase this to train longer\n",
    "batch_size = 4  # Experiment between 1 and 64\n",
    "seq_length = 100  # Experiment between 50 and 500\n",
    "learning_rate = 5e-3  # Experiment between 1e-5 and 1e-1\n",
    "\n",
    "# Model parameters\n",
    "rnn_units = 1024\n",
    "dense_units = 256\n",
    "embedding_dim = 256\n",
    "num_categories = 10\n",
    "\n",
    "# Checkpoint location: \n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"my_ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(rnn_units=rnn_units, dense_units=dense_units, output_units=num_categories)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y): \n",
    "  with tf.GradientTape() as tape:\n",
    "    y_hat = model(x)\n",
    "    loss = compute_loss(y, y_hat)\n",
    "\n",
    "  grads = tape.gradient(loss, model.trainable_variables)\n",
    "  \n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "plotter = mdl.util.PeriodicPlotter(sec=2, xlabel='Iterations', ylabel='Loss')\n",
    "if hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n",
    "\n",
    "for iter in tqdm(range(num_training_iterations)):\n",
    "\n",
    "  # Grab a batch and propagate it through the network\n",
    "  x_batch, y_batch = get_batch(sentences, batch_size)\n",
    "  loss = train_step(x_batch, y_batch)\n",
    "\n",
    "  # Update the progress bar\n",
    "  history.append(loss.numpy().mean())\n",
    "  plotter.plot(history)\n",
    "\n",
    "  # Update the model with the changed weights!\n",
    "  if iter % 100 == 0:     \n",
    "    model.save_weights(checkpoint_prefix)\n",
    "    \n",
    "# Save the trained model and the weights\n",
    "model.save_weights(checkpoint_prefix)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07759dc2ae76fc822dfeac8d7ff0a5b71e04b962bfdad2606d9e679ad765db71"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
